{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Bias and Fairness in Data ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Summary of Research",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In the field of data, bias and fairness are critical topics that directly influence the quality and impact of the results. According to Bacelar's research[1] on bias and fairness in machine learning models, data bias can emerge from various sources, including how data is collected, processed, and interpreted. One key issue is sampling bias, which occurs when the dataset used for training a model does not represent the full diversity of the population. This could lead to underrepresentation or overrepresentation of certain groups, thereby affecting the generalizability of the model. Another form of bias is measurement bias, where the data does not accurately reflect the variables of interest due to incorrect labels or subjective data collection. Furthermore, historical bias can play a significant role, where the dataset reflects past social or institutional inequalities that are then perpetuated by the model's predictions. Bacelar emphasizes the importance of addressing these biases at multiple stages—from data collection to model deployment—and highlights several techniques, such as reweighting, data transformation, and fairness-aware algorithms, as effective strategies to mitigate bias.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "###  Evaluation of our Data Potential Biases",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In our project, we collected survey data from participants across different regions of Saudi Arabia. While the dataset provides valuable insights, it is important to recognize the potential biases that could influence the results. One significant concern is sampling bias. Since the survey was primarily distributed via social media, the dataset may disproportionately represent frequent social media users, potentially excluding those who are less active or do not use social media at all. Moreover, A large share of the responses were from Riyadh, which creates a geographic bias. This overrepresentation might skew the data toward the experiences and social media habits of people in Riyadh, while underrepresenting those from other regions.\n\nAdditionally, cultural and language bias may have been introduced during the translation process, as the survey was originally conducted in Arabic and later translated into English. Certain cultural nuances or interpretations may have been lost, leading to differences in how respondents understood the questions. There is also the risk of response bias, particularly with sensitive topics such as mental health and social interactions. Participants may have provided answers that they believed were socially acceptable or desirable rather than their genuine experiences. Lastly, the dataset may suffer from demographic bias, with certain age groups or regions being overrepresented, which could limit the generalizability of the findings to the broader population.\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Implications of These Biases for the Fairness and Reliability",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The potential biases present in the dataset have important implications for both the fairness and reliability of our conclusions. First, fairness could be compromised if certain groups, such as non-social media users or older individuals, are underrepresented in the data. This could lead to conclusions that are not applicable to the entire population, especially in understanding the mental health effects of social media. For example, if the dataset primarily includes younger, frequent social media users, the results may overestimate the prevalence of social media-induced anxiety, while the experiences of older or less active users may go unaccounted for. Additionally, reliability may be affected due to the aforementioned cultural and response biases. If participants from different regions or backgrounds interpreted the questions differently, this could lead to inconsistencies in the data, making it difficult to draw accurate and reliable conclusions about the overall impact of social media on mental health. Therefore, any interpretations or recommendations based on this dataset should be made with caution, acknowledging the presence of these biases.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "###  Recommendations ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To address the biases identified in this dataset, several strategies can be implemented in future data collection and analysis efforts. First, to reduce sampling bias, it is essential to diversify the channels through which the survey is distributed. Relying solely on social media platforms limits the sample to active users. Expanding distribution methods, such as using email lists or in-person interviews which would help reach a more representative sample, including those who are less active or do not use social media at all. To minimize cultural and language bias, it is recommended that future surveys undergo thorough bilingual reviews by experts who understand both the linguistic and cultural nuances. Piloting the survey in both languages could help ensure that all participants, regardless of their background, interpret the questions consistently. To combat response bias, the survey could be designed to include more neutral language and ensure anonymity, which might encourage participants to provide more honest and accurate responses. Additionally, implementing fairness-aware techniques such as reweighting or stratified sampling can help ensure that underrepresented groups are appropriately accounted for in the analysis. By adopting these strategies, future data collection efforts can lead to more balanced and reliable conclusions, improving the overall fairness of the research.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n**References:**\n\n[1] Bacelar, Marley. \"Monitoring Bias and Fairness in Machine Learning Models: A Review.\" UNICAMP, 2021. Available at: [ScienceOpen Preprints](https://www.scienceopen.com/).\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}