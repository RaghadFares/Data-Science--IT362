{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "### Summary :\n\nThe research paper \"Fairness in Data Science\" discusses the critical importance of fairness in machine learning and data science, particularly in the context of how biases can impact outcomes in various applications. It emphasizes the need for a comprehensive understanding of fairness, considering both statistical and ethical dimensions. The paper identifies various sources of bias, including data collection methods, algorithm design, and user interaction, and provides a framework for assessing fairness.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Fairness tools\n\nAs algorithms become more reliant on multiple domains, it becomes imperative to ensure that these systems operate fairly and without bias. This requires multiple interventions at both the data and algorithm levels to remove bias and ensure fairness. However, developers can become overwhelmed by the multitude of metrics and techniques currently available, making it difficult for them to keep track of all the available fairness tools. Therefore, there is a need for unified and streamlined tools that help developers test their algorithms against defined fairness standards. In this section, we will review some of the most relevant fairness tools currently available on the market or in open-access repositories. We will walk through each tool, explaining its key functions and distinguishing features, making it easier for developers and reviewers to leverage them to enhance fairness in their algorithmic systems.\n\n1- DCUBE:\nType: Open-source system\nFunction: Supports discrimination discovery in databases through classification rule mining.\nKey Feature: Assists users in interactive and iterative classification rule extraction and analysis.\n\n2- AI Fairness 360:\nType: Open-source Python toolkit\nFunction: Detects, understands, and mitigates unwanted bias.\nKey Feature: Implements over 70 bias detection metrics and 9 bias mitigation algorithms, along with metric explanations for better understanding.\n\n3-FlipTest:\nType: Black box technique\nFunction: Uncovers discrimination in classifiers by evaluating if individuals with different protected statuses would be treated differently.\nKey Feature: Utilizes optimal transport to create pairs of similar individuals to analyze the model's outcomes.\n\n4-FairTest:\nType: Python framework\nFunction: Discovers unwarranted associations between algorithm outputs and protected group attributes.\nKey Feature: Actively assists developers in debugging applications related to fairness and aggregates findings into an interpretable bug report.\n\n5-Themis:\nType: Discrimination testing platform\nFunction: Tests software for discrimination based on causal relationships between inputs and outputs.\nKey Feature: Automatically generates test suites without requiring manually written tests.\n\n6-FAT Forensics:\nType: Open-source Python toolbox\nFunction: Inspects fairness, accountability, and transparency (FAT) aspects of machine learning systems.\nKey Feature: Unifies principles of fairness, accountability, and transparency, enabling developers to inspect and test algorithms against these goals.\n\n7-What-If Tool:\nType: Visualization tool from Google\nFunction: Allows teams to examine, evaluate, and compare machine learning models visually.\nKey Feature: Enables exploration of how changes to data points affect model outcomes.\n\n8-IBM Watson OpenScale:\nType: AI lifecycle tracking tool\nFunction: Monitors outcomes of AI applications throughout their lifecycle.\nKey Feature: Aims to adapt applications to changing business situations while ensuring compliance with fairness regulations.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Reference :\"https://www.researchgate.net/publication/352021231_Fairness_in_Data_Science\"",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}